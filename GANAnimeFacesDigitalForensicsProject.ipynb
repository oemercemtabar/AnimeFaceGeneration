{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGT4uUsEx9qw"
   },
   "source": [
    "# 1.INTRODUCTION\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook belongs to Adnan Kerem AKSOY and Omer Cem TABAR, all copyrights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking whether cuda is properly existed within the device\n",
    "import torch\n",
    "dev = 'cuda' if torch.cuda.is_available() == True else 'cpu'\n",
    "if torch.cuda.is_available() == True:\n",
    "    print(\"ok\")\n",
    "device =  torch.device(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6KISpSQyE76"
   },
   "source": [
    "# 2.DATASET DOWNLOAD AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04Af_drI0u9o"
   },
   "outputs": [],
   "source": [
    "#Import all required libraried to proceed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as m\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import time as time\n",
    "from IPython import display\n",
    "from torchvision import transforms\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGSmEpXGyY59"
   },
   "source": [
    "## 2.1 HOW TO PROCESS IMAGES INTO LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Ucnheui1Y2A",
    "outputId": "dd5d8e20-a7f0-4910-95c5-9ab523c56c86"
   },
   "outputs": [],
   "source": [
    "#Load the images into numpy array\n",
    "def load_images(folder):\n",
    "    images_list = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = mpimg.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images_list.append(img)\n",
    "    return np.array(images_list)\n",
    "images_list = load_images(r'data/anime_faces_data')\n",
    "print(type(images_list))\n",
    "print(images_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ-sPvkryeo4"
   },
   "source": [
    "## 2.3 FORMAT CHECK AND PROCESSING FOR GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05i_2Jc61jH1",
    "outputId": "6e24d443-1802-477d-a1be-dd3a206ac5f9"
   },
   "outputs": [],
   "source": [
    "#Control whether all the images are at size (64*64*3)\n",
    "control_list=[]\n",
    "counter = 0\n",
    "for i in images_list:\n",
    "    if i.shape != (64, 64, 3):\n",
    "        control_list.append(i)\n",
    "print(len(control_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ua3Am0Bz1iJG",
    "outputId": "4072c497-10fb-4b8b-e224-c1eff9b204c1"
   },
   "outputs": [],
   "source": [
    "#See the image representation in order to check whether it is successfully loaded(randomly chosen)\n",
    "num = random.randint(0, 21551)\n",
    "print(\"Image number that is randomly generated\", num)\n",
    "print(\"Total number of images within the list\", len(images_list))\n",
    "print(\"ndarray representation of the randomly chosen image\",images_list[num])\n",
    "print(\"class type of the randomly chosen image\",type(images_list[num]))\n",
    "plt.imshow(images_list[num])\n",
    "\n",
    "print(\"\\n\\n\\n-----------\\n\\n\\n\",images_list[num].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSQw_uZe1oNB",
    "outputId": "b1ca2a89-07aa-40a4-c406-5a1fe2598600"
   },
   "outputs": [],
   "source": [
    "#Checking one of the images in order to analyze the range of each pixel value\n",
    "#R,G,B values are taken into consideration seperately.\n",
    "img = mpimg.imread('data/anime_faces_data/'+str(num)+'.png')\n",
    "\n",
    "#It is observed that there is no outlier value that exceed the range [0,1]\n",
    "UnNormalizedCounter = 0\n",
    "for pixelX in range(0,img.shape[0]):\n",
    "  for pixelY in range(0,img.shape[1]):\n",
    "    for channelNum in range(0,img.shape[2]):\n",
    "      if img[pixelX][pixelY][channelNum] < 0 and img[pixelX][pixelY][channelNum] > 1:\n",
    "        UnNormalizedCounter = UnNormalizedCounter + 1\n",
    "if UnNormalizedCounter > 0:\n",
    "  print(\"Values are not normalized\")\n",
    "else:\n",
    "  print(\"Values are normalized\")\n",
    "\n",
    "################################################ NORMALIZATION NOTE #########################################################\n",
    "#There is no need for the normalization of the pixel values.\n",
    "#There exist 3 different values for each pixel(denoted as R(Red), G(Green) and B(Blue) values)\n",
    "#and each of the pixel value stands between the range [0,1] which states that values are already normalized.\n",
    "#Hence there is no need for normalization for the pixel values in the images before feeding them to the GAN(Generative Adversarial Network)\n",
    "#############################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VyJPM7a11qU"
   },
   "outputs": [],
   "source": [
    "my_images = images_list\n",
    "tensor_images = torch.Tensor(my_images)\n",
    "# we are turning our data to tensors to create the pytorch dataset\n",
    "my_dataset = TensorDataset(tensor_images.permute(0, 3, 1, 2),torch.tensor(np.ones(len(images_list))))\n",
    "my_dataloader = DataLoader(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0DkAkja132F",
    "outputId": "7230b213-114e-4914-f97f-06b7409b5806"
   },
   "outputs": [],
   "source": [
    "print(tensor_images.shape)\n",
    "print(tensor_images.permute(0, 3, 1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWaYRF18yvzG"
   },
   "source": [
    "# 3.GENERATOR AND DISCRIMINATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoiCxmir1xdY"
   },
   "outputs": [],
   "source": [
    "###Hocanın örneği###\n",
    "BATCH_SIZE = 64 #64 ile başladık\n",
    "# batch size equals to 4, we can also use 8 but in some points it creates some noise in the loss funcion graph\n",
    "\n",
    "dataloader = DataLoader(my_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=os.cpu_count())\n",
    "# we make our dataloader ready to use in the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUpbXUfuy0G0"
   },
   "source": [
    "## 3.1 GENERATOR IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqnJrJor2RMe"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.fc_net = nn.Sequential(\n",
    "            nn.Linear(input_size, 256*16*16, bias=False),#used to create single layer with (#input size) input and 256*10*32\n",
    "            nn.BatchNorm1d(256*16*16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv_model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 5, bias=False, padding=2),\n",
    "            nn.BatchNorm2d((128)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 5, stride=2, bias=False, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d((64)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 5, stride=2, bias=False, padding=2, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.fc_net(x)\n",
    "        y = y.reshape((-1,256, 16, 16))\n",
    "        y = self.conv_model(y)\n",
    "#         my_generator_images.append(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acrU9-XF2UlH"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        # nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "    if classname.find('Linear') != -1:\n",
    "        #nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.2)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6SyJydF2X0k"
   },
   "outputs": [],
   "source": [
    "generator = Generator(100)\n",
    "generator.apply(weights_init)\n",
    "# assign the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouq0fRdX2aV2"
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "noise = torch.normal(0, 1, [10, 100])\n",
    "print(noise.shape)\n",
    "generated_image = generator(noise).detach()\n",
    "print(generated_image.shape)\n",
    "###########################################################\n",
    "\n",
    "plt.imshow(  generated_image.squeeze()[0].permute(1, 2, 0)  )\n",
    "print(np.array(generated_image.squeeze()[0]).shape)\n",
    "# using generator to generate a image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT4lJ5iUy3iN"
   },
   "source": [
    "## 3.2 DISCRIMINATOR IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRhVnr102cxR"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(negative_slope=1e-1,inplace=True),\n",
    "            nn.Conv2d(64, 128, 5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(negative_slope=1e-1,inplace=True),\n",
    "            nn.Conv2d(128, 128, 5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(negative_slope=1e-1,inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8192,1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "#         labels_gan = []\n",
    "\n",
    "        y = self.model(x)\n",
    "#         my_discriminator_results.append(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_01dOTIV2dcY"
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "discriminator.apply(weights_init)\n",
    "# weights have been assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhVMyjb92flx"
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "decision = discriminator(generated_image)\n",
    "###########################################################\n",
    "# discriminators result to generated image\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBZjquHX2h-7"
   },
   "outputs": [],
   "source": [
    "cross_entropy = nn.BCEWithLogitsLoss()\n",
    "Generator_loss_list = []\n",
    "Discriminator_loss_list = []\n",
    "# controll lists\n",
    "def discriminator_loss(real_output, fake_output, device):\n",
    "\n",
    "    real_loss = cross_entropy(real_output, torch.ones_like(real_output, device=device))\n",
    "    print(\"Discriminator Real_Loss:\",real_loss)\n",
    "    fake_loss = cross_entropy(fake_output, torch.zeros_like(fake_output, device=device))\n",
    "    print(\"Discriminator Fake_Loss:\",fake_loss)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    print(\"Discriminator Total_Loss:\", total_loss)\n",
    "    Discriminator_loss_list.append([real_loss,fake_loss,total_loss])\n",
    "    # loss function for discriminator\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WXFXNrU2qFc"
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output, device):\n",
    "    ###########################################################\n",
    "    gen_loss = cross_entropy(fake_output, torch.ones_like(fake_output, device=device))\n",
    "    ###########################################################\n",
    "    print(\"Generator_Loss:\",gen_loss)\n",
    "    Generator_loss_list.append([gen_loss])\n",
    "    # loss function for generator\n",
    "    return gen_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTnZ84nuy-XM"
   },
   "source": [
    "## 3.3 TRAINING STEP FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4kXARwl8D-9"
   },
   "outputs": [],
   "source": [
    "#In order to track the generated and corresponding real images we need to save them in each batch and in each epoch\n",
    "generated_images =[]\n",
    "real_images = []\n",
    "predicted_images_list = []\n",
    "raw_images_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IrwpJOm2wEJ"
   },
   "outputs": [],
   "source": [
    "gen_opt = torch.optim.Adam(generator.parameters(), lr=7e-4)\n",
    "dis_opt = torch.optim.Adam(discriminator.parameters(), lr=7e-4)\n",
    "# our generator and discriminator uses ADAM optimizator\n",
    "# we have used learning rates between 1e-3 to 1e-6\n",
    "# the most optimized one is 1e-4, this one is also is the one that creates less noise\n",
    "##toplamda 290(+100+250+75 w 16 batch size and (9 * 10^-7)=learning rate)  epoch oldu. ilk 50 epoch 10^(-4) learning rate, sonraki 20 epoch 5* 10^(-6) ardından 220 epoch 2*10^(-6) learning rate\n",
    "## en son 100 epoch yine 2*10^(-6) learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvtXPdjE2wqi"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50 # we assign 400, we can train it more but our GPU RAM is getting almost full and because of that we\n",
    "# cannot do the last steps, so we generally use 350 to 360 epochs\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 64\n",
    "seed = torch.randn([num_examples_to_generate, noise_dim], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZprJWBIW20HT"
   },
   "outputs": [],
   "source": [
    "\n",
    "# control lists\n",
    "def train_step(images, generator, discriminator, BATCH_SIZE, noise_dim, device, dis_opt, gen_opt, bool):\n",
    "    noise = torch.randn([BATCH_SIZE, noise_dim], device=device)\n",
    "    global predicted_images_list\n",
    "    global raw_images_list\n",
    "    generated_images = generator(noise)\n",
    "    if bool == True: \n",
    "        raw_images_list.append(images)\n",
    "        predicted_images_list.append(generated_images)\n",
    "#     print(images)\n",
    "#     print(len(images[0]))\n",
    "    real_output = discriminator(images)\n",
    "\n",
    "    fake_output = discriminator(generated_images.detach())\n",
    "#     argmax_real.append(discriminator(images)) # control lists\n",
    "#     argmax_fake.append(discriminator(generated_images.detach())) # controll lists\n",
    "\n",
    "\n",
    "    disc_loss = discriminator_loss(real_output, fake_output, device)\n",
    "    dis_opt.zero_grad()\n",
    "    disc_loss.backward()\n",
    "    dis_opt.step()\n",
    "\n",
    "    fake_output = discriminator(generated_images)\n",
    "    gen_loss = generator_loss(fake_output, device)\n",
    "    gen_opt.zero_grad()\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()\n",
    "    return gen_loss, disc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Yej7o1W2_Pp"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, epochs, generator, discriminator, BATCH_SIZE, noise_dim, device, dis_opt, gen_opt):\n",
    "  gloss = []\n",
    "  dloss = []\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    # training both discriminator and generator simultaneously.\n",
    "    gen_losses = []\n",
    "    disc_losses = []\n",
    "    for image_batch, _ in dataloader:\n",
    "        image_batch = image_batch.to(device)\n",
    "        if epochs - epoch <4:\n",
    "            gen_loss, disc_loss = train_step(image_batch, generator, discriminator, BATCH_SIZE, noise_dim, device, dis_opt, gen_opt, True)\n",
    "        else: \n",
    "            gen_loss, disc_loss = train_step(image_batch, generator, discriminator, BATCH_SIZE, noise_dim, device, dis_opt, gen_opt, False)\n",
    "        gen_losses.append(gen_loss.detach().cpu())\n",
    "        disc_losses.append(disc_loss.detach().cpu())\n",
    "\n",
    "    gloss.append(np.mean(gen_losses))\n",
    "    dloss.append(np.mean(disc_losses))\n",
    "    #Produce images\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed,False,discriminator)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed,False,discriminator)\n",
    "  return gloss, dloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiZn-uUezBxf"
   },
   "source": [
    "## 3.4 SAVE OBTAINED IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hgDAgZH25xZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_and_save_images(model, epoch, test_input,boolean,discri):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_input).detach().cpu()* 250\n",
    "\n",
    "    grid = make_grid(predictions, 8).numpy().squeeze().transpose(1, 2, 0)\n",
    "    print(type(grid))\n",
    "\n",
    "    plt.imshow(grid.astype(np.uint16))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "    if boolean == True:\n",
    "        # if the boolean is True than in this part we get the predictions and use our discriminator to label them\n",
    "        # with our labeling, we show the transposed images we have created individually.\n",
    "        predictions = model(test_input)\n",
    "#         print(predictions,\"\\n\\n\\n\")\n",
    "#         print(len(predictions),\"\\n\\n\\n\")\n",
    "        predictions32 = predictions[:32]\n",
    "#         print(\"\\n\\n\",predictions32,\"\\n\\n\")\n",
    "#         print(len(predictions32))\n",
    "#         for predict in predictions32:\n",
    "\n",
    "        label_prediction=discri(predictions[:32])\n",
    "        # predictions preparation\n",
    "\n",
    "        list_label_predictions = []\n",
    "#         for i in label_prediction:\n",
    "#             list_label_predictions.append(torch.argmax(i))\n",
    "            # labeling part\n",
    "\n",
    "        # as result we returned the predictions and the labels of these predictions that have been assigned by the\n",
    "#         discriminator\n",
    "\n",
    "        return list_label_predictions,predictions\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8elCKjR3J-d"
   },
   "source": [
    "## 3.5 TRAINING STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pretrained model before start training\n",
    "discriminator = torch.load('data\\pretrained\\discridenemesoftmaxlast1')\n",
    "generator = torch.load('data\\pretrained\\generadenemesoftmaxlast1')\n",
    "import io\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4Gvadcr3PRA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "gloss, dloss = train(dataloader, EPOCHS, generator, discriminator, BATCH_SIZE, noise_dim, device, dis_opt, gen_opt)\n",
    "###########################################;################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# saving first and second checkpoints generatorloss to a file\n",
    "with open(\"generatorloss1\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(gloss, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving first and second checkpoints discriminatorloss to a file\n",
    "with open(\"discriminatorloss1\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(dloss, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(predicted_images_list[1010][30].detach().cpu().permute(1,2,0))\n",
    "# print(len(raw_images_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(raw_images_list[1010][30].detach().cpu().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving tensor to a file\n",
    "torch.save(predicted_images_list[1010], 'predicted_images_tensorstart.t')\n",
    "# Save to io.BytesIO buffer\n",
    "buffer = io.BytesIO()\n",
    "torch.save(predicted_images_list[1010], buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving tensor to a file\n",
    "torch.save(raw_images_list[1010], 'raw_images_liststart.t')\n",
    "# Save to io.BytesIO buffer\n",
    "buffer = io.BytesIO()\n",
    "torch.save(raw_images_list[1010], buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images_list[1009].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlRfmC403SDj"
   },
   "outputs": [],
   "source": [
    "label_predictions_generate_and_save,predictions_generate_and_save = generate_and_save_images(generator,\n",
    "                           EPOCHS,\n",
    "                           seed,True,discriminator)\n",
    "# with making boolean True, we are extracting the predictions and their labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lue3RRmU3VVp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num = 14\n",
    "label_list_string=[]\n",
    "# with this part, we turn our numeric label indicators to real string datatype indicators like we get from the timestamps part.\n",
    "for num in range(32):\n",
    "\n",
    "#     for label_name in range(len(mDoppler_labels_WR14_set)):\n",
    "#         if label_name == label_predictions_generate_and_save[num].detach().cpu().tolist():\n",
    "#             label_list_string.append(list(mDoppler_labels_WR14_set)[label_name])\n",
    "#             break\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    ready_img=predictions_generate_and_save[num].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    ready_img_up = ready_img.astype(np.float64)\n",
    "    plt.imshow(ready_img_up)\n",
    "\n",
    "#     plt.title(list(mDoppler_labels_WR14_set)[label_name])\n",
    "    plt.savefig('generated and labeled image raw {}'.format(num))\n",
    "    # we show the micro doppler maps and their labels that have been assigned by the discriminator individually\n",
    "    # you can see that our generated micro doppler signals are a little bit noisy\n",
    "#     print(label_predictions_generate_and_save[num].detach().cpu().tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsFZEhKIzS6z"
   },
   "source": [
    "# 4.GAN EVALUATION METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are to methods for the evaluation of the Generative Adversarial Network(GAN) Performance Evaluation\n",
    "In the below part you can observe the Inception Score(IS) and Fretchet ınception Distance(FID) Score measurements for the GAN\n",
    "\n",
    "Both evaluation method are using Inception Models that depends on the InceptionModelV3\n",
    "We used the pretrainde model in order to obtain the probabilistic predictions of the classes for both real and generated images\n",
    "Since the generation of the images are handled iteratively, we saved both generated and corresponding real image in each loss calculation within the discriminator point. By this mean we are calculating the corresponding score from both IS and FID perspective\n",
    "\n",
    "One problem is that, since the inception model assumes that the images are in the format of 299x299x3 with pixel values [0,255], we upscaled our images from 64x64x3 to 128x128x3 with the help of openCV version 2 with the INTERPOLATION_METHOD = INTER_CUBIC.\n",
    "\n",
    "Choice of the INTER_CUBIC comes from the idea that we want to increas our pixel sizes from 1x1 to 4x4  without any loss of generality since we want to upscale our image from 64x64 to 128x128.\n",
    "\n",
    "You can follow the dedicated function for upscaling, InceptionScore and FIDScore calculation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preperation of the Inception Model for both score calculations\n",
    "import h5py \n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "\n",
    "InceptionModel = InceptionV3(include_top=False, pooling='avg', input_shape=(128,128,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required libraries for the score calculations\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "from math import floor\n",
    "from numpy import ones\n",
    "from numpy import expand_dims\n",
    "from numpy import log\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_list_raw_Last = torch.load('raw_images_tensor1683.t')\n",
    "tf_list_generated_last = torch.load('predicted_images_tensor1683.t')\n",
    "# raw_images_list = torch.load('raw_images_liststart.t')\n",
    "# predicted_images_list = torch.load('predicted_images_tensorstart.t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_images_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turnin the 100 epoch model's batch to tensoflow to use it inside FID and IS\n",
    "tf_list_raw_list_last = []\n",
    "tf_list_generated_list_last = []\n",
    "\n",
    "for i in range(64):\n",
    "    img_upsample = cv2.resize(tf_list_generated_last[i].permute(1,2,0).detach().cpu().numpy(), dsize=(128, 128), interpolation=cv2.INTER_NEAREST)\n",
    "    # print(img_upsample.shape)\n",
    "    img_tf_v1 = tf.convert_to_tensor(img_upsample)\n",
    "    img_tf_v2 = tf.convert_to_tensor(img_tf_v1)\n",
    "    tf_list_generated_list_last.append(img_tf_v2)\n",
    "torhc_list_tensor_generated_last = tf.convert_to_tensor(tf_list_generated_list_last)\n",
    "\n",
    "for i in range(64):\n",
    "    img_upsample = cv2.resize(tf_list_raw_Last[i].permute(1,2,0).detach().cpu().numpy(), dsize=(128, 128), interpolation=cv2.INTER_NEAREST)\n",
    "    img_tf_v1 = tf.convert_to_tensor(img_upsample)\n",
    "    img_tf_v2 = tf.convert_to_tensor(img_tf_v1)\n",
    "    tf_list_raw_list_last.append(img_tf_v2)\n",
    "torhc_list_tensor_raw_last = tf.convert_to_tensor(tf_list_raw_list_last)\n",
    "# print(len(tf_list_1[0][0][0]))\n",
    "print(torhc_list_tensor_generated_last.shape)\n",
    "print(torhc_list_tensor_raw_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#turnin the pretrained model's batch to tensoflow to use it inside FID and IS\n",
    "tf_list_1 = []\n",
    "tf_list_2 = []\n",
    "\n",
    "for i in range(64):\n",
    "    image = predicted_images_list[i].detach().cpu().permute(1,2,0).numpy()\n",
    "    print(image.shape)\n",
    "    img_upsample = cv2.resize(image, dsize=(128, 128), interpolation=cv2.INTER_NEAREST)\n",
    "    # print(img_upsample.shape)\n",
    "    img_tf_v1 = tf.convert_to_tensor(img_upsample)\n",
    "    img_tf_v2 = tf.convert_to_tensor(img_tf_v1)\n",
    "    tf_list_1.append(img_tf_v2)\n",
    "torhc_list_tensor_generated_start = tf.convert_to_tensor(tf_list_1)\n",
    "\n",
    "for i in range(64):\n",
    "    img_upsample = cv2.resize(raw_images_list[i].permute(1,2,0).detach().cpu().numpy(), dsize=(128, 128), interpolation=cv2.INTER_NEAREST)\n",
    "    img_tf_v1 = tf.convert_to_tensor(img_upsample)\n",
    "    img_tf_v2 = tf.convert_to_tensor(img_tf_v1)\n",
    "    tf_list_2.append(img_tf_v2)\n",
    "torhc_list_tensor_raw_start = tf.convert_to_tensor(tf_list_2)\n",
    "# print(len(tf_list_1[0][0][0]))\n",
    "print(torhc_list_tensor2.shape)\n",
    "print(torhc_list_tensor1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torhc_list_tensor2[0]) #tensor check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFb_460uzbla"
   },
   "source": [
    "## 4.1 INCEPTION SCORE(IS) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The inception score is calculated by first using a pre-trained Inception v3 model to predict the class probabilities for each real and generated image.\n",
    "def InceptionScoreCalculation(images, number_of_splits):\n",
    "    #Predict the class probabilites for the images from the pretrained model\n",
    "    #These prediction reflect conditional probability\n",
    "    #High quality means low entropy\n",
    "    predicted_class_probabilites = InceptionModel.predict(images)\n",
    "    # enumerate splits of images/predictions\n",
    "    inception_scores = []\n",
    "    \n",
    "    #In most of the approaches for the calculation of inception score \n",
    "    #it is suggested that splitting the imageset\n",
    "    #and considering the average inception scores and standard deviations is sufficient\n",
    "    \n",
    "    for index in range(number_of_splits):\n",
    "        #Splitting and taking the conditional probabilities\n",
    "        indexStart, indexEnd = (index * floor(images.shape[0] / number_of_splits)), (index * floor(images.shape[0] / number_of_splits) + floor(images.shape[0] / number_of_splits))\n",
    "        conditional_set = predicted_class_probabilites[indexStart:indexEnd]\n",
    "        #Calculate the probability of the class\n",
    "        classProbability = expand_dims(conditional_set.mean(axis=0), 0)\n",
    "        # Calculating KL Divergence with log probabilties\n",
    "        KLDivergence = conditional_set * (log(conditional_set + 1e-3) - log(classProbability +1e-3))\n",
    "        #Summing the values of divergences\n",
    "        sumKLDivergence = KLDivergence.sum(axis=1)\n",
    "        #Averaging the KL values over sumKLDivergence\n",
    "        averageKLDivergence = mean(sumKLDivergence)\n",
    "        #Reverse the log operation\n",
    "        inceptionScore = exp(averageKLDivergence)\n",
    "        # store the inception score\n",
    "        inception_scores.append(inceptionScore)\n",
    "        #Take the average of the inception score and standard devaiton on images\n",
    "        inceptionScoreAverage, inceptionScoreStandarDeviation = mean(inception_scores), std(inception_scores)\n",
    "    return inceptionScoreAverage, inceptionScoreStandarDeviation\n",
    " \n",
    "#Calculate inception score with the given image set\n",
    "#Do not forget to choose the splits properly according to the image set size\n",
    "inceptionScoreAverage, inceptionScoreStandarDeviation = InceptionScoreCalculation(torhc_list_tensor_generated_last,64)\n",
    "print('Average Inception Score', inceptionScoreAverage)\n",
    "print('Standard Deviation of the Inception Score', inceptionScoreStandarDeviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9md04_2zibM"
   },
   "source": [
    "## 4.2 FRETCHET INCEPTION DISTANCE(FID) EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "from scipy.linalg import sqrtm\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.datasets.mnist import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-AUXplPWokI"
   },
   "outputs": [],
   "source": [
    "############################################# PART FOR THE GAN EVALUATION METRICS ########################################################\n",
    "#This part corresponds to the objective GAN evaluation metrics which are IS(Inception Score) and FID(Fretchet Inception Distannce)\n",
    "##########################################################################################################################################\n",
    "# example of calculating the frechet inception distance in Keras\n",
    "\n",
    "# calculate frechet inception distance\n",
    "def FIDScoreCalculation(inceptionModel, image1, image2):\n",
    "    \n",
    "    #Calculate the class prediction probabilities for the first image\n",
    "    classProbabilitiesImage1 = inceptionModel.predict(image1)\n",
    "    \n",
    "    #Calculate the class prediction probabilities for the first image\n",
    "    classProbabilitiesImage2 = inceptionModel.predict(image2)\n",
    "    \n",
    "    #Calculate the mean and the covarinces of the images\n",
    "    meanFirstImage, StandardDeviationFirstImage = classProbabilitiesImage1.mean(axis=0), cov(classProbabilitiesImage1, rowvar=False)\n",
    "    meanSecondImage, StandardDeviationSecondImage = classProbabilitiesImage2.mean(axis=0), cov(classProbabilitiesImage2, rowvar=False)\n",
    "\n",
    "    #Calculate the sum of squared differences between means\n",
    "    sumOfSqauredDistanceMean = numpy.sum((meanFirstImage - meanSecondImage)**2.0)\n",
    "    \n",
    "    #Calculate the square root of product between covariances\n",
    "    meanOfTheCovariances = sqrtm(StandardDeviationFirstImage.dot(StandardDeviationSecondImage))\n",
    "    \n",
    "    #Check whether the imaginary numbers are in correct format after the square root\n",
    "    if iscomplexobj(meanOfTheCovariances):\n",
    "        meanOfTheCovariances = meanOfTheCovariances.real\n",
    "    \n",
    "    #Calculate FID Score\n",
    "    FIDScore = sumOfSqauredDistanceMean + trace(StandardDeviationFirstImage + StandardDeviationSecondImage - 2.0 * meanOfTheCovariances)\n",
    "    return FIDScore\n",
    " \n",
    "\n",
    "# fid between images1 and images1\n",
    "fid = FIDScoreCalculation(InceptionModel, torhc_list_tensor2.numpy(), torhc_list_tensor2.numpy())\n",
    "print('If the image sets are same FID score will be', fid)\n",
    "# fid between images1 and images2\n",
    "fid = FIDScoreCalculation(InceptionModel,torhc_list_tensor2.numpy(), torhc_list_tensor1.numpy())\n",
    "print('If the image sets are different FID score will be', fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOlOu-l9zoyf"
   },
   "source": [
    "# 5.ANALYSIS ON RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhvCwpkrzxR7"
   },
   "source": [
    "## 5.1 LOSS COMPARISONS - CONVERGENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efC10fu_3Yg4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(gloss, label=\"Generator loss\")\n",
    "plt.plot(dloss, label=\"Discriminator loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "by7D-hfovrZI"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# reading the loss of two checkpoints\n",
    "with open('generatorloss', 'rb') as f:\n",
    "    generatorloss0 = pickle.load(f)\n",
    "with open('generatorloss1', 'rb') as f:\n",
    "    generatorloss1 = pickle.load(f)\n",
    "with open('discriminatorloss', 'rb') as f:\n",
    "    discriminatorloss0 = pickle.load(f)\n",
    "with open('discriminatorloss1', 'rb') as f:\n",
    "    discriminatorloss1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bblF42dSe56"
   },
   "outputs": [],
   "source": [
    "# combining losses of two checkpoints\n",
    "for i in generatorloss1:\n",
    "    generatorloss0.append(i)\n",
    "for i in discriminatorloss1:\n",
    "    discriminatorloss0.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the combined losses\n",
    "# print(generatorloss0)\n",
    "# print(generatorloss1)\n",
    "plt.plot(generatorloss0,c='r',label=\"Generator Loss\")\n",
    "plt.plot(discriminatorloss0, c= 'b',label=\"Discriminator Loss\")\n",
    "plt.legend(loc='upper right')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "yT4lJ5iUy3iN"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

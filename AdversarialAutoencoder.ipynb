{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision as tv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torch.autograd import Variable\n",
    "# from torchsummary import summary\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8729ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the images into numpy array\n",
    "def load_images(folder):\n",
    "    images_list = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = mpimg.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images_list.append(img)\n",
    "    \n",
    "    return np.array(images_list)\n",
    "images_list = load_images(r'data\\anime_faces_data')\n",
    "print(images_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a16d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_images = images_list\n",
    "tensor_images = torch.Tensor(my_images)\n",
    "# we are turning our data to tensors to create the pytorch dataset\n",
    "my_dataset = TensorDataset(tensor_images.permute(0, 3, 1, 2),torch.tensor(np.ones(len(images_list))))\n",
    "my_dataloader = DataLoader(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking whether cuda is properly existed within the device\n",
    "dev = 'cuda' if torch.cuda.is_available() == True else 'cpu'\n",
    "if torch.cuda.is_available() == True:\n",
    "    print(\"ok\")\n",
    "device =  torch.device(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeatedly reduce the size an ecode it\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=1, padding=2), # -> N, 16, 14, 14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.05, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=1, padding=2), # -> N, 32, 7, 7\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.05, inplace=True),\n",
    "            nn.Conv2d(128, 256, 3), # -> N, 64, 1, 1\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1048576,128)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        # print(\"nooluyo olumencoder\")\n",
    "        return encoded\n",
    "    \n",
    "# Input [-1, +1] -> use nn.Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed803b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unflatten(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Unflatten, self).__init__()\n",
    "        self.shape = shape\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.view(len(input), self.shape[0], self.shape[1], self.shape[2])\n",
    "# repeatedly increases the size of the encoded image and tries to recontstruct it properly\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=16*16*256*8),\n",
    "            nn.BatchNorm1d(num_features=16*16*256*8),\n",
    "            nn.LeakyReLU(0.05, inplace=True),\n",
    "\n",
    "            #we have used this solution, otherwise we couldn't jump to the next step.\n",
    "            Unflatten((128, 64, 64)),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=1,padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.05, inplace=True),\n",
    "            \n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.05, inplace=True),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(32, 3, 3, stride=1,padding=2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        # print(\"nooluyo olumdecoder\")\n",
    "        return self.net(input)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a204be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the encoded images\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        # print(\"nooluyo olumdiscriminator\")\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending the network classes to GPU and assigning them to variables. \n",
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting learning rates and assigning these learning rates to the classes.\n",
    "encoder_optimization = torch.optim.Adam(encoder.parameters(), lr=3e-4)\n",
    "decoder_optimization = torch.optim.Adam(decoder.parameters(), lr=3e-4)\n",
    "discriminator_optimization = torch.optim.Adam(discriminator.parameters(), lr=8e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d022949",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 128\n",
    "# image_size = 32\n",
    "# dataroot = '/raid/artem/tmp/celeba'\n",
    "# transform = tv.transforms.Compose([tv.transforms.Resize(image_size),\n",
    "#                                 tv.transforms.CenterCrop(image_size),\n",
    "#                                 tv.transforms.ToTensor(),\n",
    "#                                 tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# dataset = tv.datasets.ImageFolder(root=dataroot, transform = transform)\n",
    "dataloader = torch.utils.data.DataLoader(my_dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=4, drop_last=True)\n",
    "print(len(dataloader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-ceremony",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "criterion_gan = nn.BCELoss()\n",
    "criterion_rec = nn.MSELoss(reduction='mean')\n",
    "criterion_l1 = nn.L1Loss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dac75c-0125-4cd1-8617-f9b55a76e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = torch.load('data\\pretrained\\AAEdiscriminatorlast')\n",
    "encoder = torch.load('data\\pretrained\\AAEencoderlast')\n",
    "decoder = torch.load('data\\pretrained\\AAEdecoderlast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d5202f-9a24-487b-a803-66bacf3e4709",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_losses = []\n",
    "decoder_losses = []\n",
    "discriminator_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15be064-56bc-4065-9ff3-c6387189add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images_batch_list=[]\n",
    "reconstructed_images_batch_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6cc4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counter = 0\n",
    "epochs = 1\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start = time.time()\n",
    "    for index, (data, _) in enumerate(dataloader):\n",
    "        counter = counter + 1\n",
    "        batch_size = data.shape[0]\n",
    "        \n",
    "        image = Variable(data).to(device)\n",
    "        encoded_image = encoder(image)\n",
    "        \n",
    "        # x=image\n",
    "        # z = encoded_image\n",
    "        # z_real = encoded_image_real\n",
    "        # y_real = ones_dis\n",
    "        # y_fake = zeros_dis\n",
    "        \n",
    "        encoded_image_real = Variable(torch.randn(batch_size, 128).to(device))\n",
    "        ones_dis = Variable(torch.ones(batch_size).to(device))\n",
    "        discriminator_real_loss = criterion_gan(discriminator(encoded_image_real).view(-1), ones_dis)\n",
    "        \n",
    "        zeros_dis = Variable(torch.zeros(batch_size).to(device))\n",
    "        discriminator_fake_loss = criterion_gan(discriminator(encoded_image).view(-1), zeros_dis)\n",
    "        \n",
    "        discriminator.zero_grad()\n",
    "        discriminator_loss = discriminator_fake_loss + discriminator_real_loss\n",
    "        discriminator_loss.backward(retain_graph=True)\n",
    "        discriminator_optimization.step()\n",
    "        discriminator_losses.append(discriminator_loss.item())\n",
    "        \n",
    "        zeros_dis = Variable(torch.ones(batch_size).to(device))\n",
    "        encoder_gan_loss = criterion_gan(discriminator(encoded_image).view(-1), ones_dis)\n",
    "        \n",
    "        encoder_loss = encoder_gan_loss\n",
    "        \n",
    "        encoder.zero_grad()\n",
    "        encoder_loss.backward(retain_graph=True)\n",
    "        encoder_optimization.step()\n",
    "        encoder_losses.append(encoder_loss.item())\n",
    "        \n",
    "        reconstructed_image = decoder(encoded_image)\n",
    "        if counter > 165:\n",
    "            raw_images_batch_list.append(data)\n",
    "            reconstructed_images_batch_list.append(reconstructed_image)\n",
    "        decoder_reconstructed_loss = criterion_l1(image, reconstructed_image)\n",
    "       \n",
    "        decoder_loss = decoder_reconstructed_loss\n",
    "        decoder.zero_grad()\n",
    "        decoder_loss.backward()\n",
    "        decoder_optimization.step()\n",
    "        decoder_losses.append(decoder_loss.item())\n",
    "        \n",
    "        print(counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c266845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing raw image grid\n",
    "index, (data, _) = next(enumerate(dataloader))\n",
    "raw_images = Variable(data).to(device)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(tv.utils.make_grid(raw_images.detach().cpu(), nrow=16, normalize=True).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364dc5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing reconstructed image grid\n",
    "raw_images_encoded = encoder(raw_images)\n",
    "reconstructed_images = decoder(raw_images_encoded)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(tv.utils.make_grid(reconstructed_images.detach().cpu(),  nrow=16, normalize=True).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21bd3f8-124f-425e-ac80-bfe00575974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing reconstructed noise grid \n",
    "noise = Variable(torch.randn(batch_size, 128).to(device))\n",
    "reconstructed_1 = decoder(noise)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(tv.utils.make_grid(reconstructed_1.detach().cpu(),  nrow=16, normalize=True).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86f66d-f50e-4026-b874-3c86c0a74420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfe190e-a92a-4887-a9af-4ea11e84f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving reconstructed images tensor to a file\n",
    "torch.save(reconstructed_images_batch_list[2], 'AAEreconstructed_images_tensorlast.t')\n",
    "# Save to io.BytesIO buffer\n",
    "buffer = io.BytesIO()\n",
    "torch.save(reconstructed_images_batch_list[2], buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444cd93e-3551-464f-b7a1-2c0354f200ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving raw images tensor to a file\n",
    "torch.save(raw_images_batch_list[2], 'AAEraw_images_tensorlast.t')\n",
    "# Save to io.BytesIO buffer\n",
    "buffer = io.BytesIO()\n",
    "torch.save(raw_images_batch_list[2], buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preperation of the Inception Model for both score calculations\n",
    "import h5py \n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "InceptionModel = InceptionV3(include_top=False, pooling='avg', input_shape=(128,128,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28c0eb-33d5-4798-8c4d-ce256c54b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required libraries for the score calculations\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "from math import floor\n",
    "from numpy import ones\n",
    "from numpy import expand_dims\n",
    "from numpy import log\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f80977-dd38-4d49-9f82-66457764673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading saved tensors\n",
    "tf_list_raw_Last_start = torch.load('AAEraw_images_tensorstart.t')\n",
    "tf_list_generated_last_start = torch.load('AAEreconstructed_images_tensorstart.t')\n",
    "# we are loading the tensors that we saved to compare both the tensors we have saved before and the tensors we have saved while training the network\n",
    "# in this file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3ca28-b141-4ae6-9a9c-5da0b36df388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turnin the 1 epoch model's batch to tensoflow to use it inside FID and IS\n",
    "tf_list_raw_list_last = []\n",
    "tf_list_reconstructed_list_last = []\n",
    "\n",
    "for i in range(128):\n",
    "    img_upsample = cv2.resize(reconstructed_images_batch_list[2][i].permute(1,2,0).detach().cpu().numpy(), dsize=(128, 128), interpolation=cv2.INTER_NEAREST)\n",
    "    # print(img_upsample.shape)\n",
    "    img_tf_v1 = tf.convert_to_tensor(img_upsample)\n",
    "    img_tf_v2 = tf.convert_to_tensor(img_tf_v1)\n",
    "    tf_list_reconstructed_list_last.append(img_tf_v2)\n",
    "torhc_list_tensor_generated_start = tf.convert_to_tensor(tf_list_reconstructed_list_last)\n",
    "\n",
    "for i in range(128):\n",
    "    img_upsample = cv2.resize(raw_images_batch_list[2][i].permute(1,2,0).detach().cpu().numpy(), dsize=(128, 128), interpolation=cv2.INTER_NEAREST)\n",
    "    img_tf_v1 = tf.convert_to_tensor(img_upsample)\n",
    "    img_tf_v2 = tf.convert_to_tensor(img_tf_v1)\n",
    "    tf_list_raw_list_last.append(img_tf_v2)\n",
    "torhc_list_tensor_raw_start = tf.convert_to_tensor(tf_list_raw_list_last)\n",
    "# print(len(tf_list_1[0][0][0]))\n",
    "print(torhc_list_tensor_generated_last.shape)\n",
    "print(torhc_list_tensor_raw_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e86fa-d763-4a4a-b9fe-8bbedf8fb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turnin the 10 epoch model's batch to tensoflow to use it inside FID and IS\n",
    "tf_list_raw_list_start = []\n",
    "tf_list_reconstructed_list_start = []\n",
    "\n",
    "for i in range(128):\n",
    "    img_upsample = cv2.resize(tf_list_generated_last_start[i].permute(1,2,0).detach().cpu().numpy(), dsize=(128, 128), interpolation=cv2.INTER_NEAREST)\n",
    "    # print(img_upsample.shape)\n",
    "    img_tf_v1 = tf.convert_to_tensor(img_upsample)\n",
    "    img_tf_v2 = tf.convert_to_tensor(img_tf_v1)\n",
    "    tf_list_reconstructed_list_start.append(img_tf_v2)\n",
    "torhc_list_tensor_generated_last = tf.convert_to_tensor(tf_list_reconstructed_list_start)\n",
    "\n",
    "for i in range(128):\n",
    "    img_upsample = cv2.resize(tf_list_raw_Last_start[i].permute(1,2,0).detach().cpu().numpy(), dsize=(128, 128), interpolation=cv2.INTER_NEAREST)\n",
    "    img_tf_v1 = tf.convert_to_tensor(img_upsample)\n",
    "    img_tf_v2 = tf.convert_to_tensor(img_tf_v1)\n",
    "    tf_list_raw_list_start.append(img_tf_v2)\n",
    "torhc_list_tensor_raw_last = tf.convert_to_tensor(tf_list_raw_list_start)\n",
    "# print(len(tf_list_1[0][0][0]))\n",
    "print(torhc_list_tensor_generated_last.shape)\n",
    "print(torhc_list_tensor_raw_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23aab2a-b638-4a9d-aa82-69b2979aa8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The inception score is calculated by first using a pre-trained Inception v3 model to predict the class probabilities for each real and generated image.\n",
    "def InceptionScoreCalculation(images, number_of_splits):\n",
    "    #Predict the class probabilites for the images from the pretrained model\n",
    "    #These prediction reflect conditional probability\n",
    "    #High quality means low entropy\n",
    "    predicted_class_probabilites = InceptionModel.predict(images)\n",
    "    # enumerate splits of images/predictions\n",
    "    inception_scores = []\n",
    "    \n",
    "    #In most of the approaches for the calculation of inception score \n",
    "    #it is suggested that splitting the imageset\n",
    "    #and considering the average inception scores and standard deviations is sufficient\n",
    "    \n",
    "    for index in range(number_of_splits):\n",
    "        #Splitting and taking the conditional probabilities\n",
    "        indexStart, indexEnd = (index * floor(images.shape[0] / number_of_splits)), (index * floor(images.shape[0] / number_of_splits) + floor(images.shape[0] / number_of_splits))\n",
    "        conditional_set = predicted_class_probabilites[indexStart:indexEnd]\n",
    "        #Calculate the probability of the class\n",
    "        classProbability = expand_dims(conditional_set.mean(axis=0), 0)\n",
    "        # Calculating KL Divergence with log probabilties\n",
    "        KLDivergence = conditional_set * (log(conditional_set + 1e-3) - log(classProbability +1e-3))\n",
    "        #Summing the values of divergences\n",
    "        sumKLDivergence = KLDivergence.sum(axis=1)\n",
    "        #Averaging the KL values over sumKLDivergence\n",
    "        averageKLDivergence = mean(sumKLDivergence)\n",
    "        #Reverse the log operation\n",
    "        inceptionScore = exp(averageKLDivergence)\n",
    "        # store the inception score\n",
    "        inception_scores.append(inceptionScore)\n",
    "        #Take the average of the inception score and standard devaiton on images\n",
    "        inceptionScoreAverage, inceptionScoreStandarDeviation = mean(inception_scores), std(inception_scores)\n",
    "    return inceptionScoreAverage, inceptionScoreStandarDeviation\n",
    " \n",
    "#Calculate inception score with the given image set\n",
    "#Do not forget to choose the splits properly according to the image set size\n",
    "inceptionScoreAverage, inceptionScoreStandarDeviation = InceptionScoreCalculation(torhc_list_tensor_generated_last,128)\n",
    "print('Average Inception Score', inceptionScoreAverage)\n",
    "print('Standard Deviation of the Inception Score', inceptionScoreStandarDeviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1697027f-9290-40b9-a263-8d4609c90bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# PART FOR THE GAN EVALUATION METRICS ########################################################\n",
    "#This part corresponds to the objective GAN evaluation metrics which are IS(Inception Score) and FID(Fretchet Inception Distannce)\n",
    "##########################################################################################################################################\n",
    "# example of calculating the frechet inception distance in Keras\n",
    "\n",
    "# calculate frechet inception distance\n",
    "def FIDScoreCalculation(inceptionModel, image1, image2):\n",
    "    \n",
    "    #Calculate the class prediction probabilities for the first image\n",
    "    classProbabilitiesImage1 = inceptionModel.predict(image1)\n",
    "    \n",
    "    #Calculate the class prediction probabilities for the first image\n",
    "    classProbabilitiesImage2 = inceptionModel.predict(image2)\n",
    "    \n",
    "    #Calculate the mean and the covarinces of the images\n",
    "    meanFirstImage, StandardDeviationFirstImage = classProbabilitiesImage1.mean(axis=0), cov(classProbabilitiesImage1, rowvar=False)\n",
    "    meanSecondImage, StandardDeviationSecondImage = classProbabilitiesImage2.mean(axis=0), cov(classProbabilitiesImage2, rowvar=False)\n",
    "\n",
    "    #Calculate the sum of squared differences between means\n",
    "    sumOfSqauredDistanceMean = numpy.sum((meanFirstImage - meanSecondImage)**2.0)\n",
    "    \n",
    "    #Calculate the square root of product between covariances\n",
    "    meanOfTheCovariances = sqrtm(StandardDeviationFirstImage.dot(StandardDeviationSecondImage))\n",
    "    \n",
    "    #Check whether the imaginary numbers are in correct format after the square root\n",
    "    if iscomplexobj(meanOfTheCovariances):\n",
    "        meanOfTheCovariances = meanOfTheCovariances.real\n",
    "    \n",
    "    #Calculate FID Score\n",
    "    FIDScore = sumOfSqauredDistanceMean + trace(StandardDeviationFirstImage + StandardDeviationSecondImage - 2.0 * meanOfTheCovariances)\n",
    "    return FIDScore\n",
    " \n",
    "\n",
    "# fid between images1 and images1\n",
    "fid = FIDScoreCalculation(InceptionModel, torhc_list_tensor2.numpy(), torhc_list_tensor2.numpy())\n",
    "print('If the image sets are same FID score will be', fid)\n",
    "# fid between images1 and images2\n",
    "fid = FIDScoreCalculation(InceptionModel,torhc_list_tensor2.numpy(), torhc_list_tensor1.numpy())\n",
    "print('If the image sets are different FID score will be', fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the pretrained model\n",
    "torch.save(discriminator,'data\\pretrained\\AAEdiscriminatorlast')\n",
    "torch.save(encoder,'data\\pretrained\\AAEencoderlast')\n",
    "torch.save(decoder,'data\\pretrained\\AAEdecoderlast')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
